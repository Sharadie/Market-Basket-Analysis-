{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFdhSxKHOCv2"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Upload the file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# STEP 2: Load the file\n",
        "import pandas as pd\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(filename)\n",
        "print(\"Preview of data:\")\n",
        "print(df.head())\n",
        "\n",
        "# STEP 3: Text vectorization of aisle names\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Vectorize the aisle names\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['aisle'])\n",
        "\n",
        "# STEP 4: Clustering the aisle names\n",
        "k = 5  # number of clusters\n",
        "model = KMeans(n_clusters=k, random_state=42)\n",
        "df['Cluster'] = model.fit_predict(X)\n",
        "\n",
        "# STEP 5: PCA to reduce to 2D for plotting\n",
        "pca = PCA(n_components=2)\n",
        "components = pca.fit_transform(X.toarray())\n",
        "\n",
        "df['PC1'] = components[:, 0]\n",
        "df['PC2'] = components[:, 1]\n",
        "\n",
        "# Plotting the clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set2', s=100)\n",
        "plt.title(\"Clusters of Aisles Based on Name Similarity\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# STEP 6: Mock classification and heatmap for fun\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Create fake true labels (for illustration only)\n",
        "true_labels = np.random.choice(range(k), size=len(df))\n",
        "predicted_labels = df['Cluster']\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm')\n",
        "plt.title(\"Confusion Matrix Heatmap (Mocked)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, predicted_labels))\n",
        "print(\"Precision (macro):\", precision_score(true_labels, predicted_labels, average='macro'))\n",
        "print(\"Recall (macro):\", recall_score(true_labels, predicted_labels, average='macro'))\n"
      ]
    }
  ]
}